{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b1f6da",
   "metadata": {},
   "source": [
    "# Computational Linear Algebra: Homework 1\n",
    "## Pagerank algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494be046",
   "metadata": {},
   "source": [
    "**Academic Year:** 2025/2026\n",
    "\n",
    "### Team Members:\n",
    "1. Indiano, Giovanni (357942);\n",
    "2. Stradiotti, Fabio (359415)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ce85f",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "At the beginning of **World Wide Web**'s development during the 1990s, complex search engines capable of filtering vast amounts of public data to deliver relevant information were needed. Early proposed solutions struggled with accuracy, frequently returning useless or irrelevant links that didn't match the user's request. The first algorithm that actually succeeded in this goal was **PageRank**, which is the primary reason behind **Google**'s enormous success. The algorithm quantitatively rates the importance of each webpage, in order to return the most helpful results first.\n",
    "\n",
    "PageRank is a delightful application of **linear algebra**. It represents the web as a graph, in which the webpages are the vertices and the links are the edges. The intuition of the algorithm consisted in giving a certain *importance* to a page not only basing on the number of incoming links (**backlinks**), but also on the importance of the pages where these links come from. This relationship is enclosed in a matrix $A$, called the **link matrix**, where each page's importance score ($x_k$) depends on the scores of its backlinks, weighted by the number of outgoing links on those pages.\n",
    "\n",
    "From the mathematics point of view, the ranking problem consists in finding an **eigenvector** associated with a unitary eigenvalue for a **column-stochastic matrix**. While using the algorithm, it could be possible to deal with the following problems:\n",
    "\n",
    "- **Non-Unique Rankings**: if there are disconnected subwebs, the ranking might not be unique;\n",
    "- **Dangling nodes**: pages with no outgoing links (columns of zeros in the raw link matrix $A$) make the system substochastic, causing probability mass leakage.\n",
    "\n",
    "PageRank solves this problem by conceptually defining the **Google Matrix** $M$, defined as:\n",
    "\n",
    "$$\n",
    "M = (1 - m) A' + mS\n",
    "$$\n",
    "\n",
    "which is discussed in the Power Method part.\n",
    "\n",
    "In our implementation, we avoid constructing the dense matrix $M$ explicitly. Instead, we compute the matrix-vector product efficiently by treating the dangling node mass redistribution and the teleportation term as vector corrections at each iteration step.\n",
    "\n",
    "As the **Perron-Frobenius** theorem explicits, the matrix M ensures the resulting matrix is positive and column-stochastic and guarantees a unique, one-dimensional eigenspace with a positive eigenvector, providing a stable and unambiguous ranking for the entire web.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6f0ab",
   "metadata": {},
   "source": [
    "# Implementation of the PageRank Algorithm\n",
    "\n",
    "In this section, we present the implementation of the PageRank algorithm. The code is structured to handle large-scale graphs efficiently using sparse matrices.\n",
    "\n",
    "We use the **Power Iteration Method** to compute the dominant eigenvector of the Google Matrix $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d92247",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg as splinalg\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "\n",
    "# Damping factor used by Google\n",
    "m = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636cc0c",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Matrix Construction\n",
    "\n",
    "The function `read_dat` is designed to parse the graph dataset and convert it into a column-stochastic sparse matrix $A$, which serves as the core component of the PageRank computation.\n",
    "\n",
    "The process consists of three key steps:\n",
    "\n",
    "1.  **Sparse Matrix Assembly:**\n",
    "    To handle large-scale graphs efficiently, we avoid dense matrix representations ($O(N^2)$ memory). Instead, we construct the adjacency matrix using the **COO (Coordinate)** format, which stores only the non-zero entries (the edges).\n",
    "    The matrix is constructed such that $A_{ij} = 1$ if there is a link from node $j$ to node $i$. Finally, it is converted to **CSC (Compressed Sparse Column)** format, which is optimized for column slicing and vector multiplication.\n",
    "\n",
    "2.  **Column Normalization (Stochasticity):**\n",
    "    The raw adjacency matrix contains 1s and 0s. To interpret it as a transition probability matrix, we must normalize the columns so that they sum to 1.\n",
    "    The function calculates the out-degree vector $\\mathbf{d}$ (column sums) and, for every node $j$ with outgoing links ($d_j > 0$), we scale the column:\n",
    "    $$A_{ij} \\leftarrow \\frac{A_{ij}}{d_j}$$\n",
    "    \n",
    "    This is implemented efficiently using sparse matrix operations:\n",
    "    $$A_{\\text{norm}} = A \\cdot D^{-1}$$\n",
    "    where $D^{-1}$ is a diagonal matrix containing the inverse of the column sums. Nodes with no outgoing links (dangling nodes, where $d_j=0$) are explicitly handled to avoid division by zero, resulting in a zero column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de12ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat(file_name):\n",
    "    labels = {}\n",
    "    row_indices = [] # Lists to store sparse matrix coordinates\n",
    "    col_indices = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            first_line = file.readline().strip()\n",
    "            if not first_line:\n",
    "                 return None, None\n",
    "            parts = first_line.split()\n",
    "            num_nodes = int(parts[0])\n",
    "            num_edges = int(parts[1])\n",
    "            \n",
    "            # Use COO format for construction (efficient for appending)\n",
    "            # Later convert to CSC (Compressed Sparse Column) for calculation\n",
    "            \n",
    "            for _ in range(num_nodes):\n",
    "                line = file.readline().strip()\n",
    "                if line:\n",
    "                    parts = line.split(maxsplit=1) \n",
    "                    node_id = int(parts[0])\n",
    "                    node_name = parts[1]\n",
    "                    labels[node_id] = node_name\n",
    "\n",
    "            for _ in range(num_edges):\n",
    "                line = file.readline().strip()\n",
    "                if line:\n",
    "                    parts = line.split()\n",
    "                    source = int(parts[0])\n",
    "                    target = int(parts[1])\n",
    "                    # Store coordinates instead of filling dense matrix directly\n",
    "                    # A[target-1][source-1]=1\n",
    "                    row_indices.append(target - 1)\n",
    "                    col_indices.append(source - 1)\n",
    "            \n",
    "            # Create sparse matrix with 1s at specific coordinates\n",
    "            data = np.ones(len(row_indices))\n",
    "            A = sparse.coo_matrix((data, (row_indices, col_indices)), shape=(num_nodes, num_nodes)).tocsc()\n",
    "            \n",
    "            # Efficient column normalization for sparse matrix\n",
    "            # Calculate sum of each column\n",
    "            col_sums = np.array(A.sum(axis=0)).flatten()\n",
    "            \n",
    "            # Avoid division by zero. If sum is 0, scaling factor is 0.\n",
    "            # This leaves the column as a zero vector in A, which is correct because \n",
    "            # we handle the dangling mass explicitly in the power_iteration function\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                scale_factors = np.where(col_sums != 0, 1.0 / col_sums, 0)\n",
    "            \n",
    "            # Multiply A by diagonal matrix of inverse sums to normalize\n",
    "            D_inv = sparse.diags(scale_factors)\n",
    "            A = A @ D_inv\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_name}' not found.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during the analysis of the file: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    return A, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eaa108",
   "metadata": {},
   "source": [
    "## 2. The Power Iteration Method\n",
    "\n",
    "The objective is to compute the PageRank vector $\\mathbf{x}$, which corresponds to the stationary distribution of the Markov chain defined by the Google Matrix $M$. In terms of linear algebra, we seek the principal eigenvector of the matrix $M$ associated with the eigenvalue $\\lambda = 1$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = M \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Since $M$ is a column-stochastic matrix (columns sum to 1) and is primitive (due to the teleportation factor m), the **Perron-Frobenius Theorem** guarantees that this vector exists, is unique, and has strictly positive components.\n",
    "\n",
    "Mathematically, the Google Matrix is defined as:\n",
    "\n",
    "$$\n",
    "M = (1-m) A' + m S\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $A'$ is the adjusted link matrix (where columns of zeros are replaced by the uniform vector $1/n$).\n",
    "* $S$ is the teleportation matrix (a dense rank-1 matrix where every column is the personalization vector $\\mathbf{s}$).\n",
    "\n",
    "However, explicitly constructing $M$ is computationally prohibitive for large graphs (such as the Web). While the original adjacency matrix $A$ is extremely sparse, the adjusted link matrix $A'$ and the addition of $S$ makes $M$ a dense $n \\times n$ matrix.\n",
    "\n",
    "To make the algorithm efficient, we compute the matrix-vector product by exploiting the sparsity of $A$ and handling dangling nodes separately, so we won't use the matrix A'.\n",
    "\n",
    "### Handling Dangling Nodes and the Update Rule\n",
    "\n",
    "A **dangling node** (or sink node) is a page with an out-degree of zero, manifesting as a column of zeros in the sparse matrix $A$.\n",
    "\n",
    "In the context of a random walk, arriving at such a node represents an absorbing state where the surfer effectively \"disappears\". Mathematically, this causes **probability leakage**: without correction, the total probability mass of the system decreases with each iteration ($\\|\\mathbf{x}_{k+1}\\|_1 < \\|\\mathbf{x}_k\\|_1$), making it impossible to converge to a valid stationary distribution.\n",
    "\n",
    "To solve this structural issue, we treat dangling nodes as if they possess **implicit links** to all other pages in the network. Specifically, we impose a rule where the probability mass entering a dangling node is not lost, but rather **redistributed** across the entire graph according to the personalization vector $\\mathbf{s}$ (a uniform distribution $1/n$).\n",
    "\n",
    "By doing so, we consider the possibilty for a surfer to jump into any other page in the web after winding up in a dangling node, instead of ending the navigation.\n",
    "\n",
    "Considering $D$ as the set of dangling nodes, we define $w$ as the total probability mass accumulated in dangling nodes at the current iteration:\n",
    "\n",
    "$$\n",
    "w = \\sum_{j \\in \\mathcal{D}} x_j\n",
    "$$\n",
    "\n",
    "The update rule for vector $\\mathbf{x}$ at iteration $k+1$ can be decomposed into two parts:\n",
    "1.  **Link Navigation:** The mass flowing through existing links in the sparse matrix $A$.\n",
    "2.  **Redistribution (Teleportation + Dangling Correction):** The mass introduced by the damping factor $m$ plus the mass recovered from dangling nodes.\n",
    "\n",
    "The optimized update formula becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{k+1} = \\underbrace{(1-m) A \\mathbf{x}_k}_{\\text{Existing Links}} + \\underbrace{\\left[ m + (1-m)w \\right] \\mathbf{s}}_{\\text{Redistributed Mass}}\n",
    "$$\n",
    "\n",
    "This formulation ensures that probability is conserved ($\\sum x_i = 1$) at every step while keeping the computational complexity low ($O(n \\cdot Nr(A))$, where $Nr$ is the maximum number of non-zero elements per row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration_with_M(A, s, m, tolerance=1e-6, max_iterations=1000):\n",
    "    n = A.shape[0]\n",
    "    x = np.ones(n) / n # initial vector (normalized)\n",
    "    \n",
    "    # Identify dangling nodes indices (columns that sum to zero)\n",
    "    col_sums = np.array(A.sum(axis=0)).flatten()\n",
    "    dangling_indices = np.where(col_sums == 0)[0]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # 1. Compute contribution from existing links\n",
    "        # Mass from dangling nodes is lost here because their columns are 0\n",
    "        Ax = A @ x\n",
    "        \n",
    "        # Compute contribution from dangling nodes\n",
    "        dangling_contribution = np.sum(x[dangling_indices])\n",
    "        \n",
    "        # Apply the Google PageRank formula with dangling node mass redistribution\n",
    "        x_new = (1 - m) * (Ax + dangling_contribution * s) + m * s\n",
    "        \n",
    "        # Check convergence (L1 norm)\n",
    "        if np.linalg.norm(x_new - x, 1) < tolerance:\n",
    "            print(f\"  Converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "        x = x_new\n",
    "    else:\n",
    "        print(f\"  Warning: Maximum iterations ({max_iterations}) reached\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181df123",
   "metadata": {},
   "source": [
    "### Power method without M\n",
    "We also define a power method without the implementation of the Google Matrix M. This function will be useful in Exercise 12 to obtain a comparison between the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bba12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration_with_A(A, tolerance=1e-6, max_iterations=1000):\n",
    "    n = A.shape[0]\n",
    "    x = np.ones(n) / n # initial vector (normalized)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # 1. Compute contribution from existing links\n",
    "        # Mass from dangling nodes is lost here because their columns are 0\n",
    "        x_new = A @ x\n",
    "        \n",
    "        # Check convergence (L1 norm)\n",
    "        if np.linalg.norm(x_new - x, 1) < tolerance:\n",
    "            print(f\"  Converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "        x = x_new\n",
    "    else:\n",
    "        print(f\"  Warning: Maximum iterations ({max_iterations}) reached\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb26e2",
   "metadata": {},
   "source": [
    "## 3. Helper Function\n",
    "\n",
    "We define **Analyze Graph**, a wrapper to run the full analysis pipeline on a given file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d877ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_graph(filename, google_matrix=True, m=0.15, print_top_k=None):\n",
    "    print(f\"Analyzing {filename} ...\")\n",
    "    A, labels = read_dat(filename)\n",
    "    if A is None: return None, None\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    s = np.ones(n) / n # Uniform personalization vector\n",
    "    \n",
    "    # 1. Check Dangling Nodes\n",
    "    col_sums = np.array(A.sum(axis=0)).flatten()\n",
    "    dangling_indices = np.where(col_sums == 0)[0]\n",
    "    if len(dangling_indices) > 0:\n",
    "        print(f\"  - Warning: Found {len(dangling_indices)} dangling node(s).\")\n",
    "    else:\n",
    "        print(f\"  - No dangling nodes detected.\")\n",
    "    \n",
    "    # 2. Compute PageRank\n",
    "    if google_matrix:\n",
    "        x = power_iteration_with_M(A, s, m)\n",
    "    else:\n",
    "        x = power_iteration_with_A(A)\n",
    "    \n",
    "    # 3. Display Results\n",
    "    sorted_indices = np.argsort(x)[::-1]\n",
    "    \n",
    "    print(f\"  PageRank scores (Top results):\")\n",
    "    print(f\"  {'-'*40}\")\n",
    "    \n",
    "    limit = print_top_k if print_top_k else n\n",
    "    for rank, idx in enumerate(sorted_indices[:limit], 1):\n",
    "        node_label = labels[idx + 1]\n",
    "        score = x[idx]\n",
    "        print(f\"  {rank}. {node_label:20s}: {score:.6f}\")\n",
    "        \n",
    "    if print_top_k and n > print_top_k:\n",
    "        print(f\"  ... (and {n - print_top_k} more)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c25018",
   "metadata": {},
   "source": [
    "## 4. Testing on Standard Graphs\n",
    "We test the implementation on the two reference graphs provided in the paper (Figures 2.1 and 2.2) and the `hollins.dat` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Graph 1 (Paper Fig 2.1)\n",
    "analyze_graph(\"Graphs/graph1.dat\", m)\n",
    "\n",
    "# Analysis of Graph 2 (Paper Fig 2.2)\n",
    "analyze_graph(\"Graphs/graph2.dat\", m)\n",
    "\n",
    "# Analysis of Hollins Dataset\n",
    "# We limit output to top 15 results to keep the notebook clean\n",
    "analyze_graph(\"Graphs/hollins.dat\", m, print_top_k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ec166",
   "metadata": {},
   "source": [
    "## 5. Discussion of the Results\n",
    "\n",
    "The PageRank algorithm was applied to the reference graphs and the Hollins University dataset with a damping factor $m=0.15$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26039a55",
   "metadata": {},
   "source": [
    "### Analysis of Graph 1 (Figure 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"Graphs/Graph1.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6a61f",
   "metadata": {},
   "source": [
    "**Convergence:** The power iteration method converged in **19 iterations**.\n",
    "\n",
    "**Spectral Interpretation:**\n",
    "The ranking vector $x$ represents the **stationary distribution** of the random walk defined by the Google Matrix $M$.\n",
    "* **Dominant Eigenvector:** Node 1 achieves the highest score ($x_1 \\approx 0.368$). This high **eigenvector centrality** is not merely a function of in-degree (number of links) but of the quality of those links. Specifically, Node 1 receives a directed edge from Node 3 ($x_3 \\approx 0.288$), effectively absorbing a significant portion of the probability mass circulating in the network.\n",
    "* **Flow of Authority:** The hierarchy $x_1 > x_3 > x_4 > x_2$ illustrates the flow of importance: Node 3 acts as a key \"hub\" that transfers authority to Node 1. Node 2, despite being part of the strongly connected component, acts largely as a tributary, passing its mass forward without receiving high-value backlinks in return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0890b8e8",
   "metadata": {},
   "source": [
    "### Analysis of Graph 2 (Figure 2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"Graphs/Graph2.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb9e0c",
   "metadata": {},
   "source": [
    "**Convergence:** The power iteration method converged in **2 iterations**.\n",
    "\n",
    "**Spectral Interpretation:** The numerical results reveal structural equivalences within the graph, highlighting specific connectivity patterns:\n",
    "\n",
    "* **Symmetry Groups:**\n",
    "    The ranking identifies two distinct equivalence classes:\n",
    "    * **Group A ($\\{3, 4\\}$):** $x_3 = x_4 = 0.285$\n",
    "    * **Group B ($\\{1, 2\\}$):** $x_1 = x_2 = 0.200$\n",
    "\n",
    "    This implies that Nodes 3 and 4 are topologically interchangeable (they likely receive the same number of links from the same sources and point to the same targets). The same logic applies to Nodes 1 and 2. The significantly higher score of Group A suggests these nodes function as the central \"authorities\" within this small network.\n",
    "\n",
    "* **Node 5: The \"Pure Source\" Node:**\n",
    "    Node 5 has a score of exactly **0.030**. Given the parameters $n=5$ and $m=0.15$, we can mathematically isolate the origin of this value using the PageRank formula:\n",
    "\n",
    "    $$x_5 = \\underbrace{(1-m) \\sum_{j \\to 5} \\frac{x_j}{d_j}}_{\\text{Link Inflow}} + \\underbrace{\\frac{m}{n}}_{\\text{Teleportation}}$$\n",
    "\n",
    "    Since the are not **link inflow**, the importance score is  $\\frac{m}{N} = \\frac{0.15}{5} = 0.03$.\n",
    "\n",
    "    Node 5 has an **In-Degree of 0**. It is a \"Source\" node that points to others but receives no endorsements itself. It appears in the ranking solely due to the random jump mechanism; without the damping factor $m$, its score would be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4559c",
   "metadata": {},
   "source": [
    "### Analysis of Hollins University Dataset\n",
    "**Convergence:** Convergence required **58 iterations**, significantly more than the previous examples. This indicates a smaller **spectral gap** $(1 - |\\lambda_2|)$, typical of large, sparse matrices representing real-world web structures.\n",
    "\n",
    "**Structural Insights:**\n",
    "\n",
    "1.  **Dangling Nodes Handling (Mass Redistribution):**\n",
    "    The dataset contains **3189 dangling nodes** ($\\approx 53\\%$ of the graph). In the raw link matrix $A$, these nodes correspond to columns of zeros. Instead of simple re-normalization (which would bias the result towards existing strong pages, as we've mentioned before), our algorithm implements the **standard mass redistribution** method.\n",
    "    At each iteration, the probability mass $w$ that \"enters\" these dead ends is captured and redistributed uniformly to all nodes in the graph. This correctly models the behavior of a random surfer who, upon reaching a dead end, jumps to a random page rather than disappearing.\n",
    "\n",
    "2.  **Cluster Analysis:**\n",
    "    * **Global Maxima:** The homepage (`www.hollins.edu`) and top-level directories (Admissions, About) dominate the ranking. This is consistent with a \"nested\" website topology where most leaf nodes point back to the root, concentrating the steady-state probability at the top of the hierarchy.\n",
    "    * **Local Traps (Slide Galleries):** We observe a cluster of high-ranking pages related to specific academic slides (e.g., \"Sculpture/sld001.htm\"). These likely form a **tightly knit strongly connected component** (a clique of pages linking to Next/Previous). In a random walk, the surfer gets \"trapped\" in this subgraph for a long time before teleporting out, artificially inflating the local PageRank scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac168a",
   "metadata": {},
   "source": [
    "# Exercise 12: Dangling Nodes and Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b888bb6",
   "metadata": {},
   "source": [
    "**Problem Statement:**\n",
    "Add a sixth page that links to every page of the web in the previous exercise, but to which no other page links. Rank the pages using the raw link matrix $A$, then using the Google matrix $M$ with $m=0.15$, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"Graphs/Graph12.png\", width=350))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b598b255",
   "metadata": {},
   "source": [
    "This exercise demonstrates the failure of the basic eigenvector method when nodes with in-degree equal to 0 are present and how the teleporting factor $m$ resolves this issue.\n",
    "\n",
    "In the code below, we compute the importance score vector $x$ firstly using the function **Get Eigenpairs**, in order to find the eigenvector corresponding to the eigenvalue equal to 1, and then, in the second case, using our **Power Method**, characterized by the Google matrix M. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc808406",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exercise 12 Analysis:\")\n",
    "filename = \"Graphs/exercise12_graph.dat\"\n",
    "\n",
    "A, labels = read_dat(filename)\n",
    "\n",
    "if A is not None:\n",
    "    # 1. Analyze using the raw Link Matrix A\n",
    "    print(f\"--- Ranking using Matrix A (Raw Link Structure) ---\")\n",
    "    analyze_graph(filename, google_matrix=False, m=0.15)\n",
    "\n",
    "    # 2. Analyze using the Google Matrix M (m=0.15)\n",
    "    print(f\"--- Ranking using Matrix M (m=0.15) ---\")\n",
    "    analyze_graph(filename, m=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f485e4",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "\n",
    "The comparison between the raw link matrix $A$ and the Google Matrix $M$ highlights the necessity of the damping  factor $m$ for ensuring a complete ranking of the network.\n",
    "\n",
    "1. **Matrix A (Connectivity Issue):**\n",
    "   The raw model correctly identifies the topological disconnection of **Node 6**. Since the node has **no incoming links** (orphan node), it receives no endorsement from the rest of the network, resulting in a null score ($x_6 = 0.00$).\n",
    "\n",
    "2. **Matrix M (Regularization):**\n",
    "   The Google Matrix model (with $m=0.15$) solves the reachability problem via the teleportation mechanism.\n",
    "   * **Minimum Score Guarantee:** Node 6 is assigned a score of exactly $0.025$. This corresponds to the uniform redistribution of the damping probability:\n",
    "     $$\\frac{m}{n} = \\frac{0.15}{6} = 0.025$$\n",
    "     This confirms that Node 6 relies entirely on the random surfer \"jumping\" to it (teleportation) rather than incoming links.\n",
    "   * **Preservation of Hierarchy:** The relative ranking of the top nodes remains stable (Node 3 > Node 1 > Node 5), maintaining the logical structure of the dominant component of the web graph while integrating isolated nodes into the global system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0baf7",
   "metadata": {},
   "source": [
    "# Exercise 15: Convergence Rate Analysis\n",
    "\n",
    "**Problem Statement:**\n",
    "Consider an $n \\times n$ positive column-stochastic matrix $M$ that is diagonalizable. Let $x_0$ be any probability vector. Using the spectral decomposition, evaluate the limit:\n",
    "\n",
    "$$\n",
    "\\lim_{k\\to\\infty} \\frac{||M^k x_0 - q||_1}{||M^{k-1} x_0 - q||_1}\n",
    "$$\n",
    "\n",
    "where $q$ is the steady-state vector.\n",
    "\n",
    "## Proof\n",
    "\n",
    "### Hypotheses:\n",
    "1. $M$ is a positive, column-stochastic matrix ($M_{ij} > 0$).\n",
    "2. $M$ is diagonalizable.\n",
    "3. The eigenvalues are ordered such that $\\lambda_1 = 1 > |\\lambda_2| \\ge |\\lambda_3| \\ge \\dots \\ge |\\lambda_n|$.\n",
    "4. $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\}$ is a basis of eigenvectors, where $\\mathbf{v}_1 = \\mathbf{q}$ is the unique steady-state vector associated with $\\lambda_1 = 1$.\n",
    "5. $x_0$ is the initial probability vector ($\\sum (x_0)_i = 1$).\n",
    "\n",
    "### Part 1: Spectral Expansion\n",
    "Since the eigenvectors form a basis for $\\mathbb{R}^n$, we can write the initial vector $x_0$ as a linear combination:\n",
    "\n",
    "$$\n",
    "x_0 = c_1 \\mathbf{v}_1 + \\sum_{j=2}^{n} c_j \\mathbf{v}_j\n",
    "$$\n",
    "\n",
    "Applying the matrix $M$ iteratively $k$ times, and noting that $M^k \\mathbf{v}_1 = 1^k \\mathbf{q} = \\mathbf{q}$ and $M^k \\mathbf{v}_j = \\lambda_j^k \\mathbf{v}_j$:\n",
    "\n",
    "$$\n",
    "M^k x_0 = c_1 \\mathbf{q} + \\sum_{j=2}^{n} c_j \\lambda_j^k \\mathbf{v}_j\n",
    "$$\n",
    "\n",
    "### Part 2: Determining the first coefficient ($c_1=1$)\n",
    "Let $\\mathbf{e} = [1, 1, \\dots, 1]^T$. Since $M$ is column-stochastic, we have $\\mathbf{e}^T M = \\mathbf{e}^T$.\n",
    "For any eigenvector $\\mathbf{v}_j$ with $\\lambda_j \\neq 1$ (i.e., $j \\ge 2$):\n",
    "\n",
    "$$\n",
    "\\mathbf{e}^T M \\mathbf{v}_j = \\lambda_j (\\mathbf{e}^T \\mathbf{v}_j) \\implies \\mathbf{e}^T \\mathbf{v}_j = \\lambda_j (\\mathbf{e}^T \\mathbf{v}_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "(1 - \\lambda_j)(\\mathbf{e}^T \\mathbf{v}_j) = 0\n",
    "$$\n",
    "\n",
    "Since $\\lambda_j \\neq 1$, it must be that $\\mathbf{e}^T \\mathbf{v}_j = 0$. In other words, the sum of components of any non-principal eigenvector is zero.\n",
    "\n",
    "Now, check the sum of components for $x_0$:\n",
    "\n",
    "$$\n",
    "\\mathbf{e}^T x_0 = c_1 (\\mathbf{e}^T \\mathbf{q}) + \\sum_{j=2}^n c_j (\\mathbf{e}^T \\mathbf{v}_j)\n",
    "$$\n",
    "\n",
    "Since $x_0$ and $\\mathbf{q}$ are probability vectors, their sums are 1. Since $\\mathbf{e}^T \\mathbf{v}_j = 0$ for $j \\ge 2$:\n",
    "\n",
    "$$\n",
    "1 = c_1(1) + 0 \\implies c_1 = 1\n",
    "$$\n",
    "\n",
    "### Part 3: Bounding eigenvalues\n",
    "Since $M$ is a positive matrix ($M_{ij} > 0$), it is *primitive*. By the **Perron-Frobenius Theorem**, the spectral radius is 1, and 1 is the unique eigenvalue on the spectral circle. Therefore:\n",
    "\n",
    "$$\n",
    "|\\lambda_j| < 1 \\quad \\text{for all } j = 2, \\dots, n.\n",
    "$$\n",
    "\n",
    "This ensures that $\\lim_{k \\to \\infty} \\lambda_j^k = 0$.\n",
    "\n",
    "### Part 4: Evaluation of the Limit\n",
    "We evaluate the ratio of the errors. The error vector at step $k$ is:\n",
    "\n",
    "$$\n",
    "M^k x_0 - \\mathbf{q} = \\left( \\mathbf{q} + \\sum_{j=2}^{n} c_j \\lambda_j^k \\mathbf{v}_j \\right) - \\mathbf{q} = \\sum_{j=2}^{n} c_j \\lambda_j^k \\mathbf{v}_j\n",
    "$$\n",
    "\n",
    "For sufficiently large $k$, the sum is asymptotically dominated by the term associated with the largest eigenvalue magnitude less than 1, which is $|\\lambda_2|$. We assume the generic case where the initial vector $x_0$ has a non-zero component along $\\mathbf{v}_2$ (i.e., $c_2 \\neq 0$).\n",
    "\n",
    "$$\n",
    "M^k x_0 - \\mathbf{q} \\approx c_2 \\lambda_2^k \\mathbf{v}_2 \\quad \\text{as } k \\to \\infty\n",
    "$$\n",
    "\n",
    "$$\n",
    "M^{k-1} x_0 - \\mathbf{q} \\approx c_2 \\lambda_2^{k-1} \\mathbf{v}_2 \\quad \\text{as } k \\to \\infty\n",
    "$$\n",
    "\n",
    "Substituting these approximations into the limit expression:\n",
    "\n",
    "$$\n",
    "\n",
    "\\lim_{k\\to\\infty} \\frac{||M^k x_0 - q||_1}{||M^{k-1} x_0 - q||_1}=\\lim_{k\\to\\infty} \\frac{||c_2 \\lambda_2^k \\mathbf{v}_2||_1}{||c_2 \\lambda_2^{k-1} \\mathbf{v}_2||_1} = \\lim_{k\\to\\infty} \\frac{|c_2| |\\lambda_2|^k ||\\mathbf{v}_2||_1}{|c_2| |\\lambda_2|^{k-1} ||\\mathbf{v}_2||_1} = \\frac{|\\lambda_2|^k}{|\\lambda_2|^{k-1}} = |\\lambda_2|\n",
    "$$\n",
    "\n",
    "### Conclusion\n",
    "The limit evaluates to $|\\lambda_2|$. This result demonstrates that the asymptotic convergence rate of the Power Method is entirely governed by the **subdominant eigenvalue** $\\lambda_2$.\n",
    "\n",
    "Specifically:\n",
    "* The error $||\\text{error}_k||_1$ decreases by a factor of approximately $|\\lambda_2|$ at each iteration.\n",
    "* The speed of convergence depends on the **Spectral Gap** ($1 - |\\lambda_2|$). A larger gap (smaller $|\\lambda_2|$) implies fewer iterations are needed to reach a desired tolerance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
