{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b1f6da",
   "metadata": {},
   "source": [
    "# Computational Linear Algebra: PageRank Algorithm Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494be046",
   "metadata": {},
   "source": [
    "**Academic Year:** 2025/2026\n",
    "\n",
    "### Team Members:\n",
    "1. Indiano, Giovanni (357942);\n",
    "2. Stradiotti, Fabio (359415)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ce85f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "At the beginning of **World Wide Web**'s development during the 1990s, complex search engines capable of filtering vast amounts of public data to deliver relevant information were needed. Early proposed solutions struggled with accuracy, frequently returning useless or irrelevant links that didn't match the user's request. The first algorithm that actually succeeded in this goal was **PageRank**, which is the primary reason behind **Google**'s enormous success. The algorithm quantitatively rates the importance of each webpage, in order to return the most helpful results first.\n",
    "\n",
    "PageRank is a delightful application of **linear algebra**. It represents the web as a graph, in which the webpages are the vertices and the links are the edges. The intuition of the algorithm consisted in giving a certain *importance* to a page not only basing on the number of incoming links (**backlinks**), but also on the importance of the pages where these links come from. This relationship is enclosed in a matrix $A$, called the **link matrix**, where each page's importance score ($x_k$) depends on the scores of its backlinks, weighted by the number of outgoing links on those pages.\n",
    "\n",
    "From the mathematics point of view, the ranking problem consists in finding an **eigenvector** associated with a unitary eigenvalue for a **column-stochastic matrix**. While using the algorithm, it could be possible to deal with the following problems:\n",
    "\n",
    "- **Non-Unique Rankings**: if there are more disconnected subwebs, it's impossible to find a single unique ranking;\n",
    "- **Dangling nodes**: pages with no outgoing links make the matrix substochastic, which may lack of an eigenvalue equal to 1.\n",
    "\n",
    "PageRank solves this problem by using the **Google Matrix** $M$, defined as:\n",
    "\n",
    "$$\n",
    "M = (1 - m)A + mS\n",
    "$$\n",
    "\n",
    "where $S$ is an *egalitarian* matrix, with entries equal to $\\frac{1}{n}$, and $m$ is a real number such that $0 \\le m \\le 1$. As the **Perron-Frobenius** theorem explicits, this modification ensures the resulting matrix is positive and column-stochastic and guarantees a unique, one-dimensional eigenspace with a positive eigenvector, providing a stable and unambiguous ranking for the entire web.\n",
    "\n",
    "This report presents our proposed solutions for the exercises included in the assignment basing on linear algebra principles, such as the development of the link matrix, the convergence of the power method, and the impact of web structure on page importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6f0ab",
   "metadata": {},
   "source": [
    "# Implementation of the PageRank Algorithm\n",
    "\n",
    "In this section, we present the implementation of the PageRank algorithm. The code is structured to handle large-scale graphs efficiently using sparse matrices.\n",
    "\n",
    "We use the **Power Iteration Method** to compute the dominant eigenvector of the Google Matrix $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d92247",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178c2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg as splinalg\n",
    "\n",
    "# Damping factor used by Google\n",
    "m = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636cc0c",
   "metadata": {},
   "source": [
    "# 1. Graph Reading and Matrix Construction\n",
    "\n",
    "The function `read_dat` reads the graph structure from a file. To handle large graphs efficiently (like the web), we use **sparse matrices** (specifically the Compressed Sparse Column format, CSC).\n",
    "\n",
    "The Link Matrix $A$ is constructed as follows:\n",
    "1.  For each link $j \\to i$, we place a $1$ in $A_{ij}$.\n",
    "2.  We normalize the columns so that $A$ becomes **column-stochastic** (the sum of each column is 1). This is done by multiplying $A$ by the inverse of the diagonal matrix of column sums: $A = A D^{-1}$.\n",
    "\n",
    "If a node has no outgoing links (dangling node), its column sum is 0, which we handle gracefully to avoid division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de12ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat(file_name):\n",
    "    \"\"\"\n",
    "    Reads a graph from a .dat file and constructs the column-stochastic link matrix A.\n",
    "    Returns:\n",
    "        A (scipy.sparse.csc_matrix): The link matrix.\n",
    "        labels (dict): A mapping from node ID to node name.\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    row_indices = [] # Lists to store sparse matrix coordinates\n",
    "    col_indices = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            first_line = file.readline().strip()\n",
    "            if not first_line:\n",
    "                 return None, None\n",
    "            parts = first_line.split()\n",
    "            num_nodes = int(parts[0])\n",
    "            num_edges = int(parts[1])\n",
    "            \n",
    "            # Use COO format for construction (efficient for appending)\n",
    "            # Later convert to CSC (Compressed Sparse Column) for calculation\n",
    "            \n",
    "            for _ in range(num_nodes):\n",
    "                line = file.readline().strip()\n",
    "                if line:\n",
    "                    parts = line.split(maxsplit=1) \n",
    "                    node_id = int(parts[0])\n",
    "                    node_name = parts[1]\n",
    "                    labels[node_id] = node_name\n",
    "\n",
    "            for _ in range(num_edges):\n",
    "                line = file.readline().strip()\n",
    "                if line:\n",
    "                    parts = line.split()\n",
    "                    source = int(parts[0])\n",
    "                    target = int(parts[1])\n",
    "                    # Store coordinates instead of filling dense matrix directly\n",
    "                    # A[target-1][source-1]=1\n",
    "                    row_indices.append(target - 1)\n",
    "                    col_indices.append(source - 1)\n",
    "            \n",
    "            # Create sparse matrix with 1s at specific coordinates\n",
    "            data = np.ones(len(row_indices))\n",
    "            A = sparse.coo_matrix((data, (row_indices, col_indices)), shape=(num_nodes, num_nodes)).tocsc()\n",
    "            \n",
    "            # Efficient column normalization for sparse matrix\n",
    "            # Calculate sum of each column\n",
    "            col_sums = np.array(A.sum(axis=0)).flatten()\n",
    "            \n",
    "            # Avoid division by zero. If sum is 0, scaling factor is 0.\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                scale_factors = np.where(col_sums != 0, 1.0 / col_sums, 0)\n",
    "            \n",
    "            # Multiply A by diagonal matrix of inverse sums to normalize\n",
    "            D_inv = sparse.diags(scale_factors)\n",
    "            A = A @ D_inv\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_name}' not found.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during the analysis of the file: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    return A, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eaa108",
   "metadata": {},
   "source": [
    "## 2. The Power Iteration Method\n",
    "\n",
    "To find the PageRank vector $x$, we need to find the stationary distribution of the Markov chain defined by the Google Matrix $M$:\n",
    "$$M = (1-m)A + mS$$\n",
    "where $S$ is the rank-one matrix $\\mathbf{e}\\mathbf{v}^T$ (with $\\mathbf{v}$ usually being the uniform vector $1/n$).\n",
    "\n",
    "Instead of explicitly constructing the dense matrix $M$ (which would destroy sparsity), we compute the matrix-vector multiplication exploitng the structure:\n",
    "$$x^{(k+1)} = M x^{(k)} = (1-m) A x^{(k)} + m S x^{(k)}$$\n",
    "\n",
    "Since $S x^{(k)} = \\mathbf{e}(\\mathbf{v}^T x^{(k)}) = \\mathbf{e} (1) = \\mathbf{e}$ (assuming $x$ is normalized), the update rule becomes:\n",
    "$$x^{(k+1)} = (1-m) A x^{(k)} + m \\mathbf{v}$$\n",
    "where $\\mathbf{v}$ is the vector of uniform probabilities ($1/n$).\n",
    "\n",
    "We iterate until the norm of the difference between consecutive vectors is below a tolerance $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2286dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration_with_vector(A, s, m, tolerance=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Computes the PageRank vector using the Power Iteration method.\n",
    "    Args:\n",
    "        A: The column-stochastic link matrix (sparse).\n",
    "        s: The personalization vector (usually uniform 1/n).\n",
    "        m: The damping factor (probability of random jump).\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    x = np.ones(n) / n # initial vector (normalized)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Sparse matrix multiplication (@) is efficient here\n",
    "        # x_new = (1 - m) * A * x + m * s\n",
    "        x_new = (1 - m) * (A @ x) + m * s\n",
    "        \n",
    "        # Re-normalize to ensure numerical stability (sum should theoretically remain 1)\n",
    "        x_new = x_new / np.sum(x_new) \n",
    "        \n",
    "        # Check convergence (L1 norm)\n",
    "        if np.linalg.norm(x_new - x, 1) < tolerance:\n",
    "            print(f\"  Converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "        x = x_new\n",
    "    else:\n",
    "        print(f\"  Warning: Maximum iterations ({max_iterations}) reached\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb26e2",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "We define utility functions to:\n",
    "1.  **Check dangling nodes**: Identify pages with no outgoing links (columns of 0).\n",
    "2.  **Check stochasticity**: Verify if a matrix is column-stochastic.\n",
    "3.  **Analyze Graph**: A wrapper to run the full analysis pipeline on a given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d877ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dangling_nodes(A):\n",
    "    col_sums = np.array(A.sum(axis=0)).flatten()\n",
    "    dangling = []\n",
    "    for i in range(len(col_sums)):\n",
    "        if col_sums[i] == 0:\n",
    "            dangling.append(i)\n",
    "    return dangling\n",
    "\n",
    "def analyze_graph(filename, m=0.15, print_top_k=None):\n",
    "    \"\"\"\n",
    "    Main function to analyze a graph file.\n",
    "    Args:\n",
    "        filename: Path to the .dat file.\n",
    "        m: Damping factor.\n",
    "        print_top_k: If set, prints only the top k results (useful for large datasets).\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing {filename} ...\")\n",
    "    A, labels = read_dat(filename)\n",
    "    if A is None: return None, None\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    s = np.ones(n) / n # Uniform personalization vector\n",
    "    \n",
    "    # 1. Check Dangling Nodes\n",
    "    dangling = check_dangling_nodes(A)\n",
    "    if dangling:\n",
    "        print(f\"  - Warning: Found {len(dangling)} dangling node(s).\")\n",
    "    else:\n",
    "        print(f\"  - No dangling nodes detected.\")\n",
    "    \n",
    "    # 2. Compute PageRank\n",
    "    x = power_iteration_with_vector(A, s, m)\n",
    "    \n",
    "    # 3. Display Results\n",
    "    sorted_indices = np.argsort(x)[::-1]\n",
    "    \n",
    "    print(f\"  PageRank scores (Top results):\")\n",
    "    print(f\"  {'-'*40}\")\n",
    "    \n",
    "    limit = print_top_k if print_top_k else n\n",
    "    for rank, idx in enumerate(sorted_indices[:limit], 1):\n",
    "        node_label = labels[idx + 1]\n",
    "        score = x[idx]\n",
    "        print(f\"  {rank}. {node_label:20s}: {score:.6f}\")\n",
    "        \n",
    "    if print_top_k and n > print_top_k:\n",
    "        print(f\"  ... (and {n - print_top_k} more)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c25018",
   "metadata": {},
   "source": [
    "## 4. Testing on Standard Graphs\n",
    "\n",
    "As requested, we test the implementation on the two reference graphs provided in the paper (Figures 2.1 and 2.2) and the `hollins.dat` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a3efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Graphs/graph1.dat ...\n",
      "  - No dangling nodes detected.\n",
      "  Converged in 19 iterations\n",
      "  PageRank scores (Top results):\n",
      "  ----------------------------------------\n",
      "  1. Node1               : 0.368151\n",
      "  2. Node3               : 0.287962\n",
      "  3. Node4               : 0.202078\n",
      "  4. Node2               : 0.141809\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analyzing Graphs/graph2.dat ...\n",
      "  - No dangling nodes detected.\n",
      "  Converged in 2 iterations\n",
      "  PageRank scores (Top results):\n",
      "  ----------------------------------------\n",
      "  1. Node4               : 0.285000\n",
      "  2. Node3               : 0.285000\n",
      "  3. Node2               : 0.200000\n",
      "  4. Node1               : 0.200000\n",
      "  5. Node5               : 0.030000\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analyzing Graphs/hollins.dat ...\n",
      "  - Warning: Found 3189 dangling node(s).\n",
      "  Converged in 206 iterations\n",
      "  PageRank scores (Top results):\n",
      "  ----------------------------------------\n",
      "  1. http://www.hollins.edu/: 0.017669\n",
      "  2. http://www.hollins.edu/admissions/visit/visit.htm: 0.010238\n",
      "  3. http://www.hollins.edu/about/about_tour.htm: 0.009457\n",
      "  4. http://www.hollins.edu/htdig/index.html: 0.009069\n",
      "  5. http://www.hollins.edu/admissions/info-request/info-request.cfm: 0.008857\n",
      "  6. http://www1.hollins.edu/faculty/saloweyca/clas%20395/Sculpture/sld001.htm: 0.008121\n",
      "  7. http://www.hollins.edu/admissions/apply/apply.htm: 0.007805\n",
      "  8. http://www.hollins.edu/admissions/admissions.htm: 0.006934\n",
      "  9. http://www1.hollins.edu/faculty/saloweyca/clas%20395/Sculpture/index.htm: 0.006924\n",
      "  10. http://www1.hollins.edu/faculty/saloweyca/clas%20395/Sculpture/tsld001.htm: 0.006319\n",
      "  11. http://www1.hollins.edu/faculty/saloweyca/clas%20395/Sculpture/sld053.htm: 0.006134\n",
      "  12. http://www1.hollins.edu/faculty/saloweyca/clas%20395/BronzeAGe/sld001.htm: 0.006113\n",
      "  13. http://www.hollins.edu/academics/academics.htm: 0.005922\n",
      "  14. http://www.hollins.edu/academics/library/resources/web_linx.htm: 0.005259\n",
      "  15. http://www1.hollins.edu/faculty/saloweyca/clas%20395/BronzeAGe/index.htm: 0.005190\n",
      "  ... (and 5997 more)\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysis of Graph 1 (Paper Fig 2.1)\n",
    "analyze_graph(\"Graphs/graph1.dat\", m)\n",
    "\n",
    "# Analysis of Graph 2 (Paper Fig 2.2)\n",
    "analyze_graph(\"Graphs/graph2.dat\", m)\n",
    "\n",
    "# Analysis of Hollins Dataset\n",
    "# We limit output to top 15 results to keep the notebook clean\n",
    "analyze_graph(\"Graphs/hollins.dat\", m, print_top_k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ec166",
   "metadata": {},
   "source": [
    "## 5. Discussion of the Results\n",
    "\n",
    "The PageRank algorithm was applied to the reference graphs and the Hollins University dataset with a damping factor $m=0.15$. The results are interpreted below through the lens of spectral graph theory and Markov chains.\n",
    "\n",
    "### Analysis of Graph 1 (Figure 2.1)\n",
    "**Convergence:** The power iteration method converged in **19 iterations** to a residual norm $< 10^{-6}$.\n",
    "\n",
    "**Spectral Interpretation:**\n",
    "The ranking vector $x$ represents the **stationary distribution** of the random walk defined by the Google Matrix $M$.\n",
    "* **Dominant Eigenvector:** Node 1 achieves the highest score ($x_1 \\approx 0.368$). This high **eigenvector centrality** is not merely a function of in-degree (number of links) but of the quality of those links. Specifically, Node 1 receives a directed edge from Node 3 ($x_3 \\approx 0.288$), effectively absorbing a significant portion of the probability mass circulating in the network.\n",
    "* **Flow of Authority:** The hierarchy $x_1 > x_3 > x_4 > x_2$ illustrates the flow of importance: Node 3 acts as a key \"hub\" that transfers authority to Node 1. Node 2, despite being part of the strongly connected component, acts largely as a tributary, passing its mass forward without receiving high-value backlinks in return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb9e0c",
   "metadata": {},
   "source": [
    "### Analysis of Graph 2 (Figure 2.2)\n",
    "**Convergence:** The algorithm converged rapidly in **2 iterations**. This implies that the second largest eigenvalue modulus $|\\lambda_2|$ of the matrix $M$ is very small, leading to a fast decay of the transient error component.\n",
    "\n",
    "**Topological Analysis:**\n",
    "The ranking highlights the graph's automorphisms and connectivity issues:\n",
    "\n",
    "1.  **Symmetries and Automorphisms:**\n",
    "    The pairs $\\{3, 4\\}$ and $\\{1, 2\\}$ exhibit identical scores ($x_3=x_4 \\approx 0.285$, $x_1=x_2 \\approx 0.200$). This confirms that the graph possesses structural symmetries where these nodes are topologically indistinguishable (i.e., swapping Node 3 with Node 4 leaves the adjacency matrix invariant).\n",
    "\n",
    "2.  **The Isolated Node (Node 5):**\n",
    "    Node 5 is structurally disconnected from the main component (it has indegree 0 from the rest of the graph). Its score of **0.030** is derived exclusively from the **teleportation term**:\n",
    "    $$x_5 = \\frac{m}{N} \\cdot \\sum_{j} x_j = \\frac{0.15}{5} \\cdot 1 = 0.03$$\n",
    "    This result validates the robustness of the PageRank formulation: without the damping factor $m$ (i.e., if $m=0$), the matrix $A$ would be reducible, and Node 5 might have a score of 0, potentially causing convergence issues depending on the initial vector. The term $mS$ ensures $M$ is strictly positive (primitive), guaranteeing a unique positive eigenvector (Perron-Frobenius)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4559c",
   "metadata": {},
   "source": [
    "### Analysis of Hollins University Dataset\n",
    "**Convergence:** Convergence required **206 iterations**, significantly more than the toy examples. This indicates a smaller **spectral gap** $(1 - |\\lambda_2|)$, typical of large, sparse matrices representing real-world web structures.\n",
    "\n",
    "**Structural Insights:**\n",
    "\n",
    "1.  **Dangling Nodes Handling:**\n",
    "    The dataset contains **3189 dangling nodes** ($\\approx 53\\%$ of the graph). These are pages with out-degree 0 (sinks). The algorithm implicitly handles these by replacing their zero columns in $A$ with the uniform vector $\\mathbf{v} = \\frac{1}{N}\\mathbf{1}$, essentially treating them as linking to everyone. This prevents the \"rank sink\" effect where probability mass would otherwise be trapped and lost.\n",
    "\n",
    "2.  **Cluster Analysis:**\n",
    "    * **Global Maxima:** The homepage (`www.hollins.edu`) and top-level directories (Admissions, About) dominate the ranking. This is consistent with a \"nested\" website topology where most leaf nodes point back to the root, concentrating the steady-state probability at the top of the hierarchy.\n",
    "    * **Local Traps (Slide Galleries):** We observe a cluster of high-ranking pages related to specific academic slides (e.g., \"Sculpture/sld001.htm\"). These likely form a **tightly knit strongly connected component** (a clique of pages linking to Next/Previous). In a random walk, the surfer gets \"trapped\" in this subgraph for a long time before teleporting out, artificially inflating the local PageRank scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
