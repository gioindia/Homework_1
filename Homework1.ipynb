{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b1f6da",
   "metadata": {},
   "source": [
    "# Computational Linear Algebra: Homework 1\n",
    "## Pagerank algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494be046",
   "metadata": {},
   "source": [
    "**Academic Year:** 2025/2026\n",
    "\n",
    "### Team Members:\n",
    "1. Indiano, Giovanni (357942);\n",
    "2. Stradiotti, Fabio (359415)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ce85f",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "At the beginning of **World Wide Web**'s development during the 1990s, complex search engines capable of filtering vast amounts of public data to deliver relevant information were needed. Early proposed solutions struggled with accuracy, frequently returning useless or irrelevant links that didn't match the user's request. The first algorithm that actually succeeded in this goal was **PageRank**, which is the primary reason behind **Google**'s enormous success. The algorithm quantitatively rates the importance of each webpage, in order to return the most helpful results first.\n",
    "\n",
    "PageRank is a delightful application of **linear algebra**. It represents the web as a graph, in which the webpages are the vertices and the links are the edges. The intuition of the algorithm consisted in giving a certain *importance* to a page not only basing on the number of incoming links (**backlinks**), but also on the importance of the pages where these links come from. This relationship is enclosed in a matrix $A$, called the **link matrix**, where each page's importance score ($x_k$) depends on the scores of its backlinks, weighted by the number of outgoing links on those pages.\n",
    "\n",
    "From the mathematics point of view, the ranking problem consists in finding an **eigenvector** associated with a unitary eigenvalue for a **column-stochastic matrix**. While using the algorithm, it could be possible to deal with the following problems:\n",
    "\n",
    "- **Non-Unique Rankings**: if there are disconnected subwebs, the ranking might not be unique;\n",
    "- **Dangling nodes**: pages with no outgoing links (columns of zeros in the raw link matrix $A$) make the system substochastic, causing probability mass leakage.\n",
    "\n",
    "PageRank solves this problem by conceptually defining the **Google Matrix** $M$. To ensure stochasticity, the raw matrix $A$ is first adjusted into a stochastic matrix $\\bar{A}$ by linking dangling nodes to all other pages (virtually). Then, the Google Matrix is defined as:\n",
    "\n",
    "$$\n",
    "M = (1 - m)\\bar{A} + mS\n",
    "$$\n",
    "\n",
    "where $S$ is the *teleportation* matrix (rank-one matrix with entries $1/n$), and $m$ is the **damping factor** ($0 \\le m \\le 1$). In our implementation, we avoid constructing the dense matrix $M$ explicitly; instead, we compute the matrix-vector product efficiently by treating the dangling node mass redistribution and the teleportation term as vector corrections at each iteration step.\n",
    "\n",
    "As the **Perron-Frobenius** theorem explicits, this modification ensures the resulting matrix is positive and column-stochastic and guarantees a unique, one-dimensional eigenspace with a positive eigenvector, providing a stable and unambiguous ranking for the entire web.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6f0ab",
   "metadata": {},
   "source": [
    "# Implementation of the PageRank Algorithm\n",
    "\n",
    "In this section, we present the implementation of the PageRank algorithm. The code is structured to handle large-scale graphs efficiently using sparse matrices.\n",
    "\n",
    "We use the **Power Iteration Method** to compute the dominant eigenvector of the Google Matrix $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d92247",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "178c2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg as splinalg\n",
    "\n",
    "# Damping factor used by Google\n",
    "m = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636cc0c",
   "metadata": {},
   "source": [
    "# 1. Graph Reading and Matrix Construction\n",
    "\n",
    "The function `read_dat` reads the graph structure from a file. To handle large graphs efficiently (like the web), we use **sparse matrices** (specifically the Compressed Sparse Column format, CSC).\n",
    "\n",
    "The Link Matrix $A$ is constructed as follows:\n",
    "1.  For each link $j \\to i$, we place a $1$ in $A_{ij}$.\n",
    "2.  We normalize the columns so that $A$ becomes **column-stochastic** (the sum of each column is 1). This is done by multiplying $A$ by the inverse of the diagonal matrix of column sums: $A = A D^{-1}$.\n",
    "\n",
    "If a node has no outgoing links (dangling node), its column sum is 0, which we handle gracefully to avoid division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9de12ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat(file_name):\n",
    "    \"\"\"\n",
    "    Reads a graph from a .dat file and constructs the column-stochastic link matrix A.\n",
    "    Returns:\n",
    "        A (scipy.sparse.csc_matrix): The link matrix.\n",
    "        labels (dict): A mapping from node ID to node name.\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    row_indices = [] # Lists to store sparse matrix coordinates\n",
    "    col_indices = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            first_line = file.readline().strip()\n",
    "            if not first_line:\n",
    "                 return None, None\n",
    "            parts = first_line.split()\n",
    "            num_nodes = int(parts[0])\n",
    "            num_edges = int(parts[1])\n",
    "            \n",
    "            # Use COO format for construction (efficient for appending)\n",
    "            # Later convert to CSC (Compressed Sparse Column) for calculation\n",
    "            \n",
    "            for _ in range(num_nodes):\n",
    "                line = file.readline().strip()\n",
    "                if line:\n",
    "                    parts = line.split(maxsplit=1) \n",
    "                    node_id = int(parts[0])\n",
    "                    node_name = parts[1]\n",
    "                    labels[node_id] = node_name\n",
    "\n",
    "            for _ in range(num_edges):\n",
    "                line = file.readline().strip()\n",
    "                if line:\n",
    "                    parts = line.split()\n",
    "                    source = int(parts[0])\n",
    "                    target = int(parts[1])\n",
    "                    # Store coordinates instead of filling dense matrix directly\n",
    "                    # A[target-1][source-1]=1\n",
    "                    row_indices.append(target - 1)\n",
    "                    col_indices.append(source - 1)\n",
    "            \n",
    "            # Create sparse matrix with 1s at specific coordinates\n",
    "            data = np.ones(len(row_indices))\n",
    "            A = sparse.coo_matrix((data, (row_indices, col_indices)), shape=(num_nodes, num_nodes)).tocsc()\n",
    "            \n",
    "            # Efficient column normalization for sparse matrix\n",
    "            # Calculate sum of each column\n",
    "            col_sums = np.array(A.sum(axis=0)).flatten()\n",
    "            \n",
    "            # Avoid division by zero. If sum is 0, scaling factor is 0.\n",
    "            # This leaves the column as a zero vector in A, which is correct because \n",
    "            # we handle the dangling mass explicitly in the power_iteration function\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                scale_factors = np.where(col_sums != 0, 1.0 / col_sums, 0)\n",
    "            \n",
    "            # Multiply A by diagonal matrix of inverse sums to normalize\n",
    "            D_inv = sparse.diags(scale_factors)\n",
    "            A = A @ D_inv\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_name}' not found.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during the analysis of the file: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    return A, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eaa108",
   "metadata": {},
   "source": [
    "## 2. The Power Iteration Method\n",
    "\n",
    "To find the PageRank vector $x$, we need to find the stationary distribution of the Markov chain defined by the Google Matrix $M$.\n",
    "\n",
    "Mathematically, for a web with dangling nodes (columns of zeros in $A$), the stochastic matrix $S$ is defined by replacing zero-columns with the uniform vector $\\mathbf{v} = 1/n$. The Google Matrix is:\n",
    "$$M = (1-m) A' + m E$$\n",
    "where $A'$ is the adjusted link matrix and $E$ is the teleportation matrix.\n",
    "\n",
    "In our implementation, we avoid constructing the dense matrix $M$. Instead, we compute the matrix-vector multiplication efficiently by handling the **mass lost** in dangling nodes explicitly.\n",
    "If $w = \\sum_{j \\in \\mathcal{D}} x_j$ is the total probability mass currently at dangling nodes, the update rule is derived as:\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = (1-m) A \\mathbf{x}_k + \\left( m + (1-m)w \\right) \\mathbf{s}$$\n",
    "\n",
    "This formula ensures that the mass entering a dangling node is not lost but is redistributed to all pages in the web (according to vector $\\mathbf{s}$), strictly conserving the total probability $\\sum x_i = 1$.\n",
    "We iterate until the norm of the difference between consecutive vectors is below a tolerance $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2286dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration_with_vector(A, s, m, tolerance=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Computes the PageRank vector using the Power Iteration method.\n",
    "    Args:\n",
    "        A: The column-stochastic link matrix (sparse).\n",
    "        s: The personalization vector (usually uniform 1/n).\n",
    "        m: The damping factor (probability of random jump).\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    x = np.ones(n) / n # initial vector (normalized)\n",
    "    \n",
    "    # Identify dangling nodes indices (columns that sum to zero)\n",
    "    col_sums = np.array(A.sum(axis=0)).flatten()\n",
    "    dangling_indices = np.where(col_sums == 0)[0]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # 1. Compute contribution from existing links\n",
    "        # Mass from dangling nodes is lost here because their columns are 0\n",
    "        Ax = A @ x\n",
    "        \n",
    "        # Compute contribution from dangling nodes\n",
    "        dangling_contribution = np.sum(x[dangling_indices])\n",
    "        \n",
    "        # Apply the Google PageRank formula with dangling node mass redistribution\n",
    "        x_new = (1 - m) * Ax + (1-m)* dangling_contribution *s + m*s\n",
    "        \n",
    "        # Check convergence (L1 norm)\n",
    "        if np.linalg.norm(x_new - x, 1) < tolerance:\n",
    "            print(f\"  Converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "        x = x_new\n",
    "    else:\n",
    "        print(f\"  Warning: Maximum iterations ({max_iterations}) reached\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb26e2",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "We define utility functions to:\n",
    "1.  **Check dangling nodes**: Identify pages with no outgoing links (columns of 0).\n",
    "2.  **Check stochasticity**: Verify if a matrix is column-stochastic.\n",
    "3.  **Analyze Graph**: A wrapper to run the full analysis pipeline on a given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4d877ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dangling_nodes(A):\n",
    "    col_sums = np.array(A.sum(axis=0)).flatten()\n",
    "    dangling = []\n",
    "    for i in range(len(col_sums)):\n",
    "        if col_sums[i] == 0:\n",
    "            dangling.append(i)\n",
    "    return dangling\n",
    "\n",
    "def analyze_graph(filename, m=0.15, print_top_k=None):\n",
    "    \"\"\"\n",
    "    Main function to analyze a graph file.\n",
    "    Args:\n",
    "        filename: Path to the .dat file.\n",
    "        m: Damping factor.\n",
    "        print_top_k: If set, prints only the top k results (useful for large datasets).\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing {filename} ...\")\n",
    "    A, labels = read_dat(filename)\n",
    "    if A is None: return None, None\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    s = np.ones(n) / n # Uniform personalization vector\n",
    "    \n",
    "    # 1. Check Dangling Nodes\n",
    "    dangling = check_dangling_nodes(A)\n",
    "    if dangling:\n",
    "        print(f\"  - Warning: Found {len(dangling)} dangling node(s).\")\n",
    "    else:\n",
    "        print(f\"  - No dangling nodes detected.\")\n",
    "    \n",
    "    # 2. Compute PageRank\n",
    "    x = power_iteration_with_vector(A, s, m)\n",
    "    \n",
    "    # 3. Display Results\n",
    "    sorted_indices = np.argsort(x)[::-1]\n",
    "    \n",
    "    print(f\"  PageRank scores (Top results):\")\n",
    "    print(f\"  {'-'*40}\")\n",
    "    \n",
    "    limit = print_top_k if print_top_k else n\n",
    "    for rank, idx in enumerate(sorted_indices[:limit], 1):\n",
    "        node_label = labels[idx + 1]\n",
    "        score = x[idx]\n",
    "        print(f\"  {rank}. {node_label:20s}: {score:.6f}\")\n",
    "        \n",
    "    if print_top_k and n > print_top_k:\n",
    "        print(f\"  ... (and {n - print_top_k} more)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c25018",
   "metadata": {},
   "source": [
    "## 4. Testing on Standard Graphs\n",
    "\n",
    "As requested, we test the implementation on the two reference graphs provided in the paper (Figures 2.1 and 2.2) and the `hollins.dat` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0a3efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Graphs/graph1.dat ...\n",
      "  - No dangling nodes detected.\n",
      "  Converged in 19 iterations\n",
      "  PageRank scores (Top results):\n",
      "  ----------------------------------------\n",
      "  1. Node1               : 0.368151\n",
      "  2. Node3               : 0.287962\n",
      "  3. Node4               : 0.202078\n",
      "  4. Node2               : 0.141809\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analyzing Graphs/graph2.dat ...\n",
      "  - No dangling nodes detected.\n",
      "  Converged in 2 iterations\n",
      "  PageRank scores (Top results):\n",
      "  ----------------------------------------\n",
      "  1. Node4               : 0.285000\n",
      "  2. Node3               : 0.285000\n",
      "  3. Node2               : 0.200000\n",
      "  4. Node1               : 0.200000\n",
      "  5. Node5               : 0.030000\n",
      "\n",
      "============================================================\n",
      "\n",
      "Analyzing Graphs/hollins.dat ...\n",
      "  - Warning: Found 3189 dangling node(s).\n",
      "  Converged in 58 iterations\n",
      "  PageRank scores (Top results):\n",
      "  ----------------------------------------\n",
      "  1. http://www.hollins.edu/: 0.019879\n",
      "  2. http://www.hollins.edu/admissions/visit/visit.htm: 0.009288\n",
      "  3. http://www.hollins.edu/about/about_tour.htm: 0.008610\n",
      "  4. http://www.hollins.edu/htdig/index.html: 0.008065\n",
      "  5. http://www.hollins.edu/admissions/info-request/info-request.cfm: 0.008027\n",
      "  6. http://www.hollins.edu/admissions/apply/apply.htm: 0.007165\n",
      "  7. http://www.hollins.edu/academics/library/resources/web_linx.htm: 0.006583\n",
      "  8. http://www.hollins.edu/admissions/admissions.htm: 0.005989\n",
      "  9. http://www.hollins.edu/academics/academics.htm: 0.005572\n",
      "  10. http://www1.hollins.edu/faculty/saloweyca/clas%20395/Sculpture/sld001.htm: 0.004452\n",
      "  11. http://www.hollins.edu/grad/coedgrad.htm: 0.004385\n",
      "  12. http://www1.hollins.edu/faculty/saloweyca/clas%20395/Sculpture/tsld001.htm: 0.003778\n",
      "  13. http://www1.hollins.edu/faculty/saloweyca/clas%20395/Sculpture/index.htm: 0.003742\n",
      "  14. http://www.hollins.edu/alumnae/alumnae.htm: 0.003526\n",
      "  15. http://www1.hollins.edu/faculty/saloweyca/clas%20395/BronzeAGe/sld001.htm: 0.003398\n",
      "  ... (and 5997 more)\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysis of Graph 1 (Paper Fig 2.1)\n",
    "analyze_graph(\"Graphs/graph1.dat\", m)\n",
    "\n",
    "# Analysis of Graph 2 (Paper Fig 2.2)\n",
    "analyze_graph(\"Graphs/graph2.dat\", m)\n",
    "\n",
    "# Analysis of Hollins Dataset\n",
    "# We limit output to top 15 results to keep the notebook clean\n",
    "analyze_graph(\"Graphs/hollins.dat\", m, print_top_k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ec166",
   "metadata": {},
   "source": [
    "## 5. Discussion of the Results\n",
    "\n",
    "The PageRank algorithm was applied to the reference graphs and the Hollins University dataset with a damping factor $m=0.15$.\n",
    "\n",
    "### Analysis of Graph 1 (Figure 2.1)\n",
    "**Convergence:** The power iteration method converged in **19 iterations** to a residual norm $< 10^{-6}$.\n",
    "\n",
    "**Spectral Interpretation:**\n",
    "The ranking vector $x$ represents the **stationary distribution** of the random walk defined by the Google Matrix $M$.\n",
    "* **Dominant Eigenvector:** Node 1 achieves the highest score ($x_1 \\approx 0.368$). This high **eigenvector centrality** is not merely a function of in-degree (number of links) but of the quality of those links. Specifically, Node 1 receives a directed edge from Node 3 ($x_3 \\approx 0.288$), effectively absorbing a significant portion of the probability mass circulating in the network.\n",
    "* **Flow of Authority:** The hierarchy $x_1 > x_3 > x_4 > x_2$ illustrates the flow of importance: Node 3 acts as a key \"hub\" that transfers authority to Node 1. Node 2, despite being part of the strongly connected component, acts largely as a tributary, passing its mass forward without receiving high-value backlinks in return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb9e0c",
   "metadata": {},
   "source": [
    "### Analysis of Graph 2 (Figure 2.2)\n",
    "**Convergence:** The algorithm converged rapidly in **2 iterations**. This implies that the second largest eigenvalue modulus $|\\lambda_2|$ of the matrix $M$ is very small, leading to a fast decay of the transient error component.\n",
    "\n",
    "**Topological Analysis:**\n",
    "The ranking highlights the graph's automorphisms and connectivity issues:\n",
    "\n",
    "1.  **Symmetries and Automorphisms:**\n",
    "    The pairs $\\{3, 4\\}$ and $\\{1, 2\\}$ exhibit identical scores ($x_3=x_4 \\approx 0.285$, $x_1=x_2 \\approx 0.200$). This confirms that the graph possesses structural symmetries where these nodes are topologically indistinguishable (i.e., swapping Node 3 with Node 4 leaves the adjacency matrix invariant).\n",
    "\n",
    "2.  **The Isolated Node (Node 5):**\n",
    "    Node 5 is structurally disconnected from the main component (it has indegree 0 from the rest of the graph). Its score of **0.030** is derived exclusively from the **teleportation term**:\n",
    "    $$x_5 = \\frac{m}{N} \\cdot \\sum_{j} x_j = \\frac{0.15}{5} \\cdot 1 = 0.03$$\n",
    "    This result validates the robustness of the PageRank formulation: without the damping factor $m$ (i.e., if $m=0$), the matrix $A$ would be reducible, and Node 5 might have a score of 0, potentially causing convergence issues depending on the initial vector. The term $mS$ ensures $M$ is strictly positive (primitive), guaranteeing a unique positive eigenvector (Perron-Frobenius)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4559c",
   "metadata": {},
   "source": [
    "### Analysis of Hollins University Dataset\n",
    "**Convergence:** Convergence required **58 iterations**, significantly more than the toy examples. This indicates a smaller **spectral gap** $(1 - |\\lambda_2|)$, typical of large, sparse matrices representing real-world web structures.\n",
    "\n",
    "**Structural Insights:**\n",
    "\n",
    "1.  **Dangling Nodes Handling (Mass Redistribution):**\n",
    "    The dataset contains **3189 dangling nodes** ($\\approx 53\\%$ of the graph). In the raw link matrix $A$, these nodes correspond to columns of zeros. Instead of simple re-normalization (which would bias the result towards existing strong pages), our algorithm implements the **standard mass redistribution** method.\n",
    "    At each iteration, the probability mass $w$ that \"enters\" these dead ends is captured and redistributed uniformly to all nodes in the graph. This correctly models the behavior of a random surfer who, upon reaching a dead end, jumps to a random page rather than disappearing.\n",
    "\n",
    "2.  **Cluster Analysis:**\n",
    "    * **Global Maxima:** The homepage (`www.hollins.edu`) and top-level directories (Admissions, About) dominate the ranking. This is consistent with a \"nested\" website topology where most leaf nodes point back to the root, concentrating the steady-state probability at the top of the hierarchy.\n",
    "    * **Local Traps (Slide Galleries):** We observe a cluster of high-ranking pages related to specific academic slides (e.g., \"Sculpture/sld001.htm\"). These likely form a **tightly knit strongly connected component** (a clique of pages linking to Next/Previous). In a random walk, the surfer gets \"trapped\" in this subgraph for a long time before teleporting out, artificially inflating the local PageRank scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b598b255",
   "metadata": {},
   "source": [
    "# Exercise 12: Dangling Nodes and Robustness\n",
    "\n",
    "**Problem Statement:**\n",
    "Add a sixth page that links to every page of the web in the previous exercise, but to which no other page links (a dangling node). Rank the pages using the raw link matrix $A$, then using the Google matrix $M$ with $m=0.15$, and compare the results.\n",
    "\n",
    "This exercise demonstrates the failure of the basic eigenvector method when dangling nodes are present and how the damping factor $m$ resolves this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb3eef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eigenpairs(A, k=None):\n",
    "    # Helper function: Use Scipy for large matrices, Numpy for small ones.\n",
    "    # Scipy eigs requires k < N-1, which fails on very small graphs (e.g., 4 nodes).\n",
    "    n = A.shape[0]\n",
    "    if k is None: k = min(n - 2, 6)\n",
    "    if k < 1: k = 1\n",
    "\n",
    "    if n < 10 or k >= n-1:\n",
    "        # Fallback to dense for small graphs or when many eigenvalues are needed\n",
    "        vals, vecs = np.linalg.eig(A.toarray())\n",
    "    else:\n",
    "        try:\n",
    "            vals, vecs = splinalg.eigs(A, k=k, which='LM')\n",
    "        except:\n",
    "            vals, vecs = np.linalg.eig(A.toarray())\n",
    "    return vals, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc808406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 12 Analysis:\n",
      "--- Ranking using Matrix A (Raw Link Structure) ---\n",
      "  1. Node3               : 0.367347\n",
      "  2. Node1               : 0.244898\n",
      "  3. Node5               : 0.183673\n",
      "  4. Node4               : 0.122449\n",
      "  5. Node2               : 0.081633\n",
      "  6. Node6               : -0.000000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Ranking using Matrix M (m=0.15) ---\n",
      "Analyzing Graphs/exercise12_graph.dat ...\n",
      "  - No dangling nodes detected.\n",
      "  Converged in 28 iterations\n",
      "  PageRank scores (Top results):\n",
      "  ----------------------------------------\n",
      "  1. Node3               : 0.340172\n",
      "  2. Node1               : 0.231212\n",
      "  3. Node5               : 0.173823\n",
      "  4. Node4               : 0.135033\n",
      "  5. Node2               : 0.094760\n",
      "  6. Node6               : 0.025000\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Exercise 12 Analysis:\")\n",
    "filename = \"Graphs/exercise12_graph.dat\"\n",
    "\n",
    "# 1. Analyze using the raw Link Matrix A\n",
    "A, labels = read_dat(filename)\n",
    "\n",
    "if A is not None:\n",
    "    print(f\"--- Ranking using Matrix A (Raw Link Structure) ---\")\n",
    "    eigenvalues, eigenvectors = get_eigenpairs(A)\n",
    "    \n",
    "    # Check for eigenvalue 1\n",
    "    idx_list = np.where(np.isclose(eigenvalues, 1))[0]\n",
    "    \n",
    "    if len(idx_list) > 0:\n",
    "        idx = idx_list[0]\n",
    "        x_raw = np.real(eigenvectors[:, idx])\n",
    "        \n",
    "        # Normalize\n",
    "        importance_score = x_raw / x_raw.sum()\n",
    "        \n",
    "        sorted_indices = np.argsort(importance_score)[::-1]\n",
    "        for rank, idx in enumerate(sorted_indices, 1):\n",
    "            node_label = labels[idx + 1]\n",
    "            score = importance_score[idx]\n",
    "            print(f\"  {rank}. {node_label:20s}: {score:.6f}\")\n",
    "    else:\n",
    "        print(\"Eigenvalue 1 not found for Matrix A (expected behavior for dangling nodes).\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    # 2. Analyze using the Google Matrix M (m=0.15)\n",
    "    print(f\"--- Ranking using Matrix M (m=0.15) ---\")\n",
    "    analyze_graph(filename, m=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f485e4",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "\n",
    "The results clearly demonstrate the limitation of using the raw link matrix $A$ versus the Google Matrix $M$.\n",
    "\n",
    "1.  **Matrix A (Failure):** The original model fails to assign valid importance scores. Specifically, the dangling node (Node 6), which has no incoming links, causes issues in the spectral properties of the matrix, or results in a score of 0.00 depending on the solver method.\n",
    "2.  **Matrix M (Success):** The modified PageRank model successfully handles the dangling node.\n",
    "    * Node 6 receives a minimal positive score of $\\frac{m}{n} \\approx 0.025$, but more importantly, it does not act as a \"black hole\".\n",
    "    * The algorithm explicitly redistributes the mass accumulated in Node 6 back to the rest of the network (via the `dangling_mass` term), preserving the global stochasticity of the system.\n",
    "    * Node 3 remains the most important page, preserving the logical structure of the web.\n",
    "    \n",
    "        \n",
    "This confirms that the damping factor $m$ transforms the problem into a strictly positive, column-stochastic matrix, guaranteeing a unique and robust ranking vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0baf7",
   "metadata": {},
   "source": [
    "# Exercise 15: Convergence Rate Analysis\n",
    "\n",
    "**Problem Statement:**\n",
    "Consider an $n \\times n$ positive column-stochastic matrix $M$ that is diagonalizable. Let $x_0$ be any probability vector. Using the spectral decomposition, evaluate the limit:\n",
    "$$\\lim_{k\\to\\infty} \\frac{||M^k x_0 - q||_1}{||M^{k-1} x_0 - q||_1}$$\n",
    "where $q$ is the steady-state vector.\n",
    "\n",
    "### Proof\n",
    "\n",
    "**Hypotheses:**\n",
    "1.  $M$ is a positive, column-stochastic matrix.\n",
    "2.  $M$ is diagonalizable.\n",
    "3.  $\\{q, v_1, \\dots, v_{n-1}\\}$ is a basis of eigenvectors, where $q$ is the unique steady-state vector associated with $\\lambda_1 = 1$.\n",
    "4.  $x_0$ is the initial probability vector ($\\sum (x_0)_i = 1$).\n",
    "\n",
    "#### Part 1: Spectral Expansion\n",
    "Since the eigenvectors form a basis, we can write the initial vector $x_0$ as a linear combination:\n",
    "$$x_0 = a \\mathbf{q} + \\sum_{j=1}^{n-1} b_j \\mathbf{v}_j$$\n",
    "\n",
    "Applying the matrix $M$ iteratively $k$ times, and noting that $M^k \\mathbf{q} = 1^k \\mathbf{q} = \\mathbf{q}$ and $M^k \\mathbf{v}_j = \\lambda_j^k \\mathbf{v}_j$:\n",
    "$$M^k x_0 = a \\mathbf{q} + \\sum_{j=1}^{n-1} b_j \\lambda_j^k \\mathbf{v}_j$$\n",
    "\n",
    "#### Part 2: Determining coefficients ($a=1$)\n",
    "Let $\\mathbf{e} = [1, 1, \\dots, 1]^T$. Since $M$ is column-stochastic, $\\mathbf{e}^T M = \\mathbf{e}^T$.\n",
    "For any eigenvector $\\mathbf{v}_j$ with $\\lambda_j \\neq 1$:\n",
    "$$\\mathbf{e}^T M \\mathbf{v}_j = \\lambda_j (\\mathbf{e}^T \\mathbf{v}_j) \\implies \\mathbf{e}^T \\mathbf{v}_j = \\lambda_j (\\mathbf{e}^T \\mathbf{v}_j)$$\n",
    "$$(1 - \\lambda_j)(\\mathbf{e}^T \\mathbf{v}_j) = 0$$\n",
    "Since $\\lambda_j \\neq 1$, it must be that $\\mathbf{e}^T \\mathbf{v}_j = \\sum (\\mathbf{v}_j)_i = 0$. The sum of components of non-principal eigenvectors is zero.\n",
    "\n",
    "Now, check the sum of components for $x_0$:\n",
    "$$\\mathbf{e}^T x_0 = a (\\mathbf{e}^T \\mathbf{q}) + \\sum b_j (\\mathbf{e}^T \\mathbf{v}_j)$$\n",
    "Since $x_0$ and $q$ are probability vectors, their sums are 1. Since $\\mathbf{e}^T \\mathbf{v}_j = 0$:\n",
    "$$1 = a(1) + 0 \\implies a = 1$$\n",
    "\n",
    "#### Part 3: Bounding eigenvalues\n",
    "According to **Proposition 4** (from the paper), for any vector $v$ such that $\\sum v_i = 0$, we have $||Mv||_1 \\le c ||v||_1$ with $c < 1$.\n",
    "Applying this to our eigenvectors $\\mathbf{v}_j$:\n",
    "$$||M \\mathbf{v}_j||_1 = ||\\lambda_j \\mathbf{v}_j||_1 = |\\lambda_j| ||\\mathbf{v}_j||_1 \\le c ||\\mathbf{v}_j||_1$$\n",
    "Dividing by $||\\mathbf{v}_j||_1$, we get $|\\lambda_j| \\le c < 1$. Thus, all eigenvalues other than 1 are strictly less than 1 in magnitude.\n",
    "\n",
    "#### Part 4: Evaluation of the Limit\n",
    "We want to evaluate the ratio of the errors. The error at step $k$ is:\n",
    "$$M^k x_0 - \\mathbf{q} = \\left( \\mathbf{q} + \\sum_{j=1}^{n-1} b_j \\lambda_j^k \\mathbf{v}_j \\right) - \\mathbf{q} = \\sum_{j=1}^{n-1} b_j \\lambda_j^k \\mathbf{v}_j$$\n",
    "\n",
    "As $k \\to \\infty$, the sum is dominated by the term associated with the **second largest eigenvalue** in magnitude, denoted as $\\lambda_2$ (assuming coefficients $b_2 \\neq 0$). The terms with smaller eigenvalues vanish much faster.\n",
    "$$M^k x_0 - \\mathbf{q} \\approx b_2 \\lambda_2^k \\mathbf{v}_2$$\n",
    "$$M^{k-1} x_0 - \\mathbf{q} \\approx b_2 \\lambda_2^{k-1} \\mathbf{v}_2$$\n",
    "\n",
    "Substituting these into the limit expression:\n",
    "$$\\lim_{k\\to\\infty} \\frac{||b_2 \\lambda_2^k \\mathbf{v}_2||_1}{||b_2 \\lambda_2^{k-1} \\mathbf{v}_2||_1} = \\lim_{k\\to\\infty} \\frac{|b_2| |\\lambda_2|^k ||\\mathbf{v}_2||_1}{|b_2| |\\lambda_2|^{k-1} ||\\mathbf{v}_2||_1} = |\\lambda_2|$$\n",
    "\n",
    "**Conclusion:**\n",
    "The convergence rate of the Power Method is determined asymptotically by the magnitude of the second largest eigenvalue, $|\\lambda_2|$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
